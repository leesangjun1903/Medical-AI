# Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI

## 1. 논문의 핵심 주장과 주요 기여

본 논문은 대규모 뇌파(EEG) 데이터를 활용하여 범용적인 뇌-컴퓨터 인터페이스(BCI) 모델을 구축하는 **Large Brain Model (LaBraM)**을 제안합니다.[1]

### 주요 기여도

**대규모 EEG 사전 훈련**: 약 20개 데이터셋에서 수집한 2,500시간 이상의 다양한 EEG 데이터로 대규모 신경 트랜스포머를 사전 훈련한 최초 연구입니다.[1]

**다양한 EEG 구성 호환성**: 유연한 공간적 및 시간적 임베딩을 통해 다양한 채널 수와 시간 길이를 가진 EEG 신호를 처리할 수 있는 통합 모델입니다.[1]

**효과적인 EEG 표현 학습**: EEG 신호의 컴팩트하고 의미 있는 표현을 제공하는 neural codebook을 정의하여 masked EEG modeling으로 사전 훈련합니다.[1]

**포괄적인 다운스트림 실험**: 네 가지 대표적인 BCI 태스크에서 모든 SOTA 방법들을 큰 차이로 능가하는 성능을 보였습니다.[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 해결 문제

기존 EEG 기반 딥러닝 모델들이 특정 데이터셋과 애플리케이션에만 특화되어 있어 **모델 규모의 제한**과 **일반화 능력 부족**이라는 문제가 있었습니다.[1]

주요 도전과제:
- **충분한 EEG 데이터 부족**: 자연어나 이미지 데이터 대비 EEG 데이터 수집의 어려움[1]
- **다양한 EEG 수집 구성**: 전극 수, 데이터 길이, 작업 설계의 차이[1]
- **효과적인 EEG 표현 학습 패러다임 부족**: 낮은 신호 대 잡음 비율과 시공간 특성 균형의 어려움[1]

### 제안 방법

#### 모델 아키텍처

**Temporal Encoder**: EEG 패치를 시간적 특징으로 인코딩하기 위해 1D 컨볼루션 블록들을 사용합니다.[1]

**Spatial & Temporal Embedding**: 각 패치에 시간적/공간적 위치 정보를 추가하는 학습 가능한 임베딩을 활용합니다:[1]

$$
\{e_{c_{ij},k} + te_k + se_{ij} | j = 1,2,...,C, k = 1,2,...,⌊t/w⌋\}
$$

[1]

**Transformer Encoder**: 안정적인 훈련을 위해 수정된 attention 메커니즘을 사용합니다:[1]

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{LN(Q)LN(K)^T}{\sqrt{d_{head}}}\right)V
$$

[1]

#### Neural Tokenizer 훈련

**Vector-quantized Neural Spectrum Prediction**: 푸리에 스펙트럼을 예측하여 의미 있는 neural tokenizer를 훈련합니다.[1]

DFT 적용:

$$
\tilde{x}^m_{c,k} = \sum_{n=1}^{N} x[n] \exp\left(-2\pi j \frac{mn}{N}\right)
$$

[1]

진폭과 위상 계산:

$$
A_m = \sqrt{\text{Re}(\tilde{x}^m_{c,k})^2 + \text{Im}(\tilde{x}^m_{c,k})^2}
$$

[1]

$$
\phi_m = \arctan\left(\frac{\text{Im}(\tilde{x}^m_{c,k})}{\text{Re}(\tilde{x}^m_{c,k})}\right)
$$

[1]

총 손실 함수:

$$
L_T = \sum_{x \in D} \sum_{i=1}^{N} \|o^A_i - A_i\|_2^2 + \|o^{\phi}_i - \phi_i\|_2^2 + \|sg(\ell_2(p_i)) - \ell_2(v_{z_i})\|_2^2 + \|\ell_2(p_i) - sg(\ell_2(v_{z_i}))\|_2^2
$$

[1]

#### LaBraM 사전 훈련

**Masked EEG Modeling**: 일부 EEG 패치를 마스킹하고 보이는 패치로부터 마스킹된 토큰을 예측합니다:[1]

$$
L_M = -\sum_{x \in D} \sum_{m_i=1} \log p(v_i | e^M)
$$

[1]

**Symmetric Masking**: 훈련 효율성을 개선하기 위한 대칭 마스킹 전략을 제안합니다:[1]

$$
L = L_M + L^{sym}_M
$$

[1]

## 3. 성능 향상 및 한계

### 성능 결과

**TUAB 데이터셋 (이상 탐지)**: 
- LaBraM-Base: Balanced Accuracy 0.8140, AUC-PR 0.8965, AUROC 0.9022
- 기존 SOTA 대비 상당한 성능 향상[1]

**TUEV 데이터셋 (이벤트 유형 분류)**:
- LaBraM-Base: Balanced Accuracy 0.6409, Cohen's Kappa 0.6637, Weighted F1 0.8312
- 특히 어려운 다중 클래스 분류에서 큰 성능 개선[1]

**모델 크기별 성능**: LaBraM-Huge (369M) > LaBraM-Large (46M) > LaBraM-Base (5.8M) 순으로 성능이 향상됩니다.[1]

### 데이터 스케일링 분석

**Base 모델**: 500시간 훈련으로도 2,500시간 모델과 유사한 성능 달성.[1]

**Large 모델**: 1,000시간 이후 성장률 둔화.[1]

**Huge 모델**: 데이터 크기 증가에 따라 지속적인 성능 향상 추세를 보이며, 최소 1만 시간 규모의 데이터에서 계속 향상될 것으로 예상됩니다.[1]

### 한계점

**데이터와 모델 규모**: 현재의 대규모 비전 모델이나 언어 모델 대비 여전히 작은 규모로, 대규모 EEG 모델 훈련 가능성을 탐색하는 첫 단계입니다.[1]

**계산 비용**: 다운스트림 태스크 적응을 위해 완전한 파인튜닝이 필요하여 계산 및 메모리 비용이 높습니다.[1]

**단일 모달리티**: 단일 EEG 데이터로만 훈련되어 다른 모달리티와의 결합 연구가 필요합니다.[1]

## 4. 일반화 성능 향상 가능성

### 교차 데이터셋 학습 능력

**다운스트림 데이터셋 포함/미포함 비교**: 사전 훈련 과정에서 다운스트림 태스크 데이터셋의 포함 여부가 성능에 큰 영향을 주지 않음을 확인하여, 모델이 **범용적인 EEG 표현**을 학습할 수 있음을 입증했습니다.[1]

### 다양한 EEG 구성 처리

**통합 아키텍처**: 패치 기반 세분화와 유연한 공간적/시간적 임베딩을 통해 임의의 채널 수와 시간 길이를 가진 EEG 신호를 처리할 수 있습니다.[1]

**Spatial Embedding의 중요성**: 공간적 임베딩 없이는 사전 훈련 시 손실이 수렴하지 않으며, 파인튜닝 시에도 상당한 성능 저하가 발생하여 공간 정보 캡처의 중요성을 확인했습니다.[1]

### 스케일링 법칙 준수

대규모 언어 모델의 스케일링 법칙을 따라 **데이터 크기와 모델 크기 증가에 따른 성능 향상**을 보이며, 특히 큰 모델일수록 더 많은 데이터에서 지속적인 개선이 가능합니다.[1]

## 5. 향후 연구에 미치는 영향과 고려사항

### 연구 영향

**패러다임 전환**: EEG 분야에서 task-specific 모델에서 **foundation model** 접근법으로의 전환을 제시합니다.[1]

**벤치마킹**: BCI 분야 최대 규모의 369M 파라미터 모델로서 향후 연구의 **성능 기준점** 역할을 할 것입니다.[1]

**방법론적 기여**: masked EEG modeling과 vector-quantized neural spectrum prediction은 다른 생리학적 신호 처리에도 응용 가능한 방법론입니다.[1]

### 향후 연구 방향

**더 큰 규모의 연구**:
1. **데이터 수집 확대**: 다양한 BCI 태스크에서 더 많은 EEG 데이터 수집하여 emergent abilities 탐구[1]
2. **효율적 학습 방법**: Adapter, prompt tuning, LoRA 등을 활용한 파인튜닝 오버헤드 감소[1]
3. **다중 모달리티**: 이미지, 언어, 음성, 기타 생리학적 신호와의 통합으로 새로운 패러다임 구축[1]

**기술적 고려사항**:
- **최적 데이터 규모**: 현재 2,500시간으로는 충분하지 않으며, 최소 1만 시간 규모의 데이터가 필요할 것으로 예상됩니다[1]
- **계산 효율성**: 대규모 모델의 실용적 배포를 위한 경량화 기법 연구 필요
- **표준화**: 다양한 EEG 수집 환경에서의 일관된 성능 보장을 위한 표준화 방법 연구

이 연구는 EEG 기반 BCI 분야에서 **대규모 사전 훈련 모델의 가능성**을 처음으로 입증하고, 향후 더 큰 규모와 더 나은 일반화 능력을 가진 모델들의 개발 방향을 제시한 중요한 기초 연구입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/0b441b5c-470e-4944-8ae4-969f1bd774bd/2405.18765v1.pdf)
